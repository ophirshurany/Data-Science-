{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h1>Introduction to Data Science - Bank Marketing Project<h1>\n",
    "\n",
    "    \n",
    "\n",
    "Team Members:\n",
    "    \n",
    "Ophir Shurany ID: 304867716\n",
    "\n",
    "Amit Shreiber ID: 200867174"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1. Load Database<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.close('all')\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.1.-1.3 Create and load dataframe<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"bank.csv\",sep='|',encoding='utf8')\n",
    "#drop duplicate data\n",
    "df = data\n",
    "df = df.drop_duplicates('Unnamed: 0',keep=False)\n",
    "#drop #rows\n",
    "df=df.drop('Unnamed: 0',axis=1)\n",
    "df_copy_original=df #Keep original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2. Data Exploration<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.1. view first 5 rows in df<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.0</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>jul</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>93.918</td>\n",
       "      <td>-42.7</td>\n",
       "      <td>4.962</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51.0</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>jun</td>\n",
       "      <td>fri</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>94.465</td>\n",
       "      <td>-41.8</td>\n",
       "      <td>4.959</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51.0</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>jun</td>\n",
       "      <td>tue</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>94.465</td>\n",
       "      <td>-41.8</td>\n",
       "      <td>4.961</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>53.0</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>thu</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.860</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40.0</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>may</td>\n",
       "      <td>tue</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>success</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>92.893</td>\n",
       "      <td>-46.2</td>\n",
       "      <td>1.344</td>\n",
       "      <td>5099.1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    age          job  marital          education default housing loan  \\\n",
       "0  39.0       admin.  married  university.degree      no      no   no   \n",
       "2  51.0   management  married  university.degree     NaN      no   no   \n",
       "4  51.0  blue-collar  married           basic.4y     NaN      no  yes   \n",
       "5  53.0     services  married        high.school     NaN      no   no   \n",
       "6  40.0  blue-collar  married           basic.6y      no      no   no   \n",
       "\n",
       "     contact month day_of_week  ...  campaign  pdays  previous     poutcome  \\\n",
       "0   cellular   jul         mon  ...       3.0    999         0  nonexistent   \n",
       "2  telephone   jun         fri  ...      10.0    999         0  nonexistent   \n",
       "4  telephone   jun         tue  ...       1.0    999         0  nonexistent   \n",
       "5  telephone   may         thu  ...       2.0    999         0  nonexistent   \n",
       "6   cellular   may         tue  ...       1.0     11         1      success   \n",
       "\n",
       "  emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  nr.employed    y  \n",
       "0          1.4          93.918          -42.7      4.962       5228.1   no  \n",
       "2          1.4          94.465          -41.8      4.959       5228.1   no  \n",
       "4          1.4          94.465          -41.8      4.961       5228.1  yes  \n",
       "5          1.1          93.994          -36.4      4.860       5191.0   no  \n",
       "6         -1.8          92.893          -46.2      1.344       5099.1   no  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.2. Presenting all columns, number of rows and type<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.3. Feature statistics for numerical categories<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms for categorial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorcial_variables =list(df.select_dtypes(include=\"object\").columns)\n",
    "for feature in categorcial_variables:\n",
    "    plt.figure((figsize=(12, 6))\n",
    "    sns.countplot(x=feature,data=df)\n",
    "    plt.title(\"Feature Histogram - \" + feature,fontsize='xx-large', fontweight='bold')\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xlabel(\"Feature Name\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms for Numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = list(df.select_dtypes(exclude=\"object\").columns)\n",
    "for feature in num_features:\n",
    "#devide for economic \n",
    "    plt.figure()\n",
    "    sns.distplot(df[feature].dropna(),kde=False)\n",
    "    plt.title(\"Feature Histogram - \" + feature,fontsize='xx-large', fontweight='bold')\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.4. Categorial manipulations:<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2.4.1 change \"yes\" or \"no\" to 1 or 0<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y'] = df.y.map(dict(yes=1, no=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2.4.2 Convert the month list to 4 binary quarters column <h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months=['jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec'];\n",
    "Q = [1,1,1,1,2,2,2,3,3,3,4,4,4];month_dic=dict(zip(months,Q))\n",
    "df['month']=df.month.replace(month_dic)\n",
    "df=pd.get_dummies(df, columns=['month'],prefix='Q')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2.4.3. convert categorial features to numeric and drop the number of variables<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#education\n",
    "df['education']=df.education.replace(['basic.6y','basic.4y', 'basic.9y'], 'basic')\n",
    "#job\n",
    "df.job.replace(['admin.', 'management'], 'administration_management', inplace=True)\n",
    "df.loc[(df['age'] > 60 ) & (df['job'] == 'administration_management' ) , 'job'] = 'retired'\n",
    "df.job.replace(['retired', 'unemployed'], 'no_active_income', inplace=True)\n",
    "df.job.replace('housemaid', 'services',inplace=True)\n",
    "df['job']=df.job.replace('entrepreneur', 'self-employed')\n",
    "# Convert other Series from yes or no to binary\n",
    "df['housing'] = df.housing.map(dict(yes=1, no=0));\n",
    "df['loan'] = df.loan.map(dict(yes=1, no=0));\n",
    "df=df.rename(columns = {'contact':'contact_by_cellular'})\n",
    "df['contact_by_cellular'] = df.contact_by_cellular.map(dict(cellular = 1, telephone = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <h2>3. Missing Values<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.1. Total NaN rows = \" + str(sum(df.isna().sum())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Present NaN % in each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(100*df.isna().sum()/df.shape[0]).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to see how values are distributed:\n",
    "#first, we convert unknown values from NaN so they will be countable as unknown:\n",
    "df['default'] = df.default.replace(np.nan,'unknown',regex=True)\n",
    "#default\n",
    "pd.crosstab(df['y'],df['default'],dropna=True).apply(lambda r: r/r.sum(), axis=1).round(4)\n",
    "#most of No are at default, so we cant really learn from it. then, it will be deleted\n",
    "df=df.drop(\"default\",axis=1)\n",
    "#loan\n",
    "df['loan'] = df.loan.replace(np.nan,'unknown',regex=True)\n",
    "pd.crosstab(df['y'],df['loan']).apply(lambda r: r/r.sum(), axis=1).round(2)\n",
    "df['loan'] = df['loan'].replace('unknown',0)\n",
    "#housing\n",
    "df['housing'] = df.housing.replace(np.nan,'unknown',regex=True)\n",
    "pd.crosstab(df['y'],df['housing']).apply(lambda r: r/r.sum(), axis=1).round(2)\n",
    "#values distribute practicly evenly, therefore we can delete uknowns:\n",
    "df = df[df.housing != \"unknown\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We think that ‘job’ is influenced \n",
    " by the ‘education’ of a person. because that, \n",
    " we can infer ‘job’ based on the education of the person.\n",
    " Moreover, since we are just filling the missing values,\n",
    " we are not much concerned about the causal inference.\n",
    " We can use the job to predict education.\n",
    "\n",
    "to infer the missing values in 'job' and 'education', we make use of the cross-tabulation between 'job' and 'education'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['education'] = df.education.replace(np.nan,'unknown',regex=True)\n",
    "df['job'] = df.job.replace(np.nan,'unknown',regex=True)\n",
    "pd.crosstab(df['job'], df['education'], rownames=['job'], colnames=['education'],margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While imputing the values for job and education, we followed the fact that\n",
    "the correlations should make real world sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#education \n",
    "#for education it makes sense to use ranking\n",
    "education_dic={'illiterate': 0,'basic' : 1,'high.school' : 2,'professional.course' : 3,'university.degree' : 4}\n",
    "df['education']=df.education.replace(education_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the cross table above we can infer the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most customers with \"basic\" education work as \"blue-collar\"\n",
    "df.loc[(df['job']=='unknown') & (df['education']==1), 'job'] = 'blue-collar'\n",
    "df.loc[(df['education']=='unknown') & (df['job']=='blue-collar'), 'education'] = 1\n",
    "#Most customers in \"services\" have a \"high.school\"  degree\n",
    "df.loc[(df['education']=='unknown') & (df['job']=='services'), 'education'] = 2\n",
    "df.loc[(df['job']=='unknown') & (df['education']==2), 'job'] = 'services'\n",
    "#Most customers with \"professional.course education work as \"technician\"\n",
    "df.loc[(df['job']=='unknown') & (df['education']==3), 'job'] = 'technician'\n",
    "df.loc[(df['education']=='unknown') & (df['job']=='technician'), 'education'] = 3\n",
    "#Most customers in \"administration_management\" have a \"university.degree\"  \n",
    "df.loc[(df['education']=='unknown') & (df['job']=='administration_management'), 'education'] = 4\n",
    "df.loc[(df['job']=='unknown') & (df['education']=='administration_management'), 'job'] = 'administration_management'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['job'], df['education'], rownames=['job'], colnames=['education'],margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute by mean value for age & campaign\n",
    "#age\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan,strategy='mean')\n",
    "df[\"age\"] = imputer.fit_transform(df[['age']])\n",
    "#campaign\n",
    "df['campaign'] = imputer.fit_transform(df[['campaign']])\n",
    "# Examine the missing values in 'pdays'\n",
    "plt.figure()\n",
    "plt.hist(df.loc[df.pdays != 999, 'pdays'])\n",
    "plt.title(\"Pdays Data Distribution Without 999\", fontsize='xx-large', fontweight='bold')\n",
    "plt.xlabel(\"Pdays\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "two varibale are connected - pdays and poutcome. Lets check their connection and how many NaN we have-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['pdays'],df['poutcome'],dropna=False,margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above table, the majority of the values for 'pdays'\n",
    "are missing. The majority of these missing values occur when the 'poutcome'\n",
    "is 'non-existent'. This means that the majority of the values in 'pdays'\n",
    "are missing because the customer was never contacted before. To deal with \n",
    "this variable, we removed the numerical variable 'pdays' and replaced it\n",
    "with categorical variables with following categories: p_days_missing, \n",
    "pdays_less_5, pdays_bet_5_15, and pdays_greater_15.\n",
    "\n",
    "Add new categorical variables to our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pdays_missing'] = 0\n",
    "df['pdays_less_5'] = 0\n",
    "df['pdays_greater_15'] = 0\n",
    "df['pdays_bet_5_15'] = 0\n",
    "df['pdays_missing'][df['pdays']==999] = 1\n",
    "df['pdays_less_5'][df['pdays']<5] = 1\n",
    "df['pdays_greater_15'][(df['pdays']>15) & (df['pdays']<999)] = 1\n",
    "df['pdays_bet_5_15'][(df['pdays']>=5)&(df['pdays']<=15)]= 1\n",
    "df= df.drop(['pdays','pdays_less_5'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have many categorical variables, dummy variables needs to be created for those vaiables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert categorical variables to dummy\n",
    "df = pd.get_dummies(df , columns = ['job', 'marital' ,'day_of_week', 'poutcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df != \"unknown\"]\n",
    "print(\"we remove all other NaNs\")\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of deleted rows = \" + str(df_copy_original.shape[0]-df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"only \"+ str(round(100*(df_copy_original.shape[0]-df.shape[0])/df.shape[0],1))+\" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finally, the total NaN rows = \" + str(sum(df.isna().sum())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4\n",
    "\n",
    "Delete Q4 in order to avoid dummy variable trap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### duration\n",
    "\n",
    "The variable “duration” will need to be dropped before we start building a predictive model\n",
    "because it highly affects the output target (e.g., if duration=0 then y=”no”). \n",
    "\n",
    "Yet, the duration is not known before a call is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop([\"duration\",\"Q_4\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.5. correlation heat map<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put the target as last feature\n",
    "y=df.y\n",
    "df=df.drop(\"y\",axis=1)\n",
    "df[\"y\"]=y\n",
    "cor = df.corr().round(1)\n",
    "mask = np.zeros_like(cor, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "plt.figure(figsize=(20, 15))\n",
    "heatmap=sns.heatmap(cor,mask=mask,annot=True,annot_kws={\"size\": 10},\n",
    "                    center=0,cmap=cmap,square=True, linewidths=.5,\n",
    "                    cbar_kws={\"shrink\": .5},yticklabels=1,xticklabels=1)\n",
    "plt.title(\"Correlation Matrix\", fontsize='xx-large', fontweight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see from heatmap that the highest correlate features (abs(0.9) and above)\n",
    "the features the economic features: **[\"nr.employes\"-\"emp.var.rate\"],[\"cons.price.idx\"-\"emp.var.rate\"]\n",
    "[\"euribor3m\"-\"emp.var.rate\"],[\"nr.employes\"-\"euribor3m\"],[\"poutcome_nonexsist-previous\"],[\"poutcome_nonexsist-poutcome_failure\"]**\n",
    "nr.employed and emp.var.rate are  *highly  corelated* and also nr.employed\n",
    "and euribor3m are highly  corelated.  \n",
    "because that we will remove emp.var.rate and euribor3m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete the features above at 1 command\n",
    "df=df.drop([\"euribor3m\",\"emp.var.rate\",\"poutcome_nonexistent\",\"marital_single\",\"pdays_missing\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy_feature_filtered=df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to see the updated correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = df.corr().round(1)\n",
    "mask = np.zeros_like(cor, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "plt.figure(figsize=(20, 15))\n",
    "heatmap=sns.heatmap(cor,mask=mask,annot=True,annot_kws={\"size\": 10},\n",
    "                    center=0,cmap=cmap,square=True, linewidths=.5,\n",
    "                    cbar_kws={\"shrink\": .5},yticklabels=1,xticklabels=1)\n",
    "plt.title(\"Updated Correlation Matrix\", fontsize='xx-large', fontweight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4. Data Normalization<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4.1. Box Plot <h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outliers: Outliers are defined as 1.5 x Q3 value (75th percentile).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_lst=[\"cons.price.idx\",\"nr.employed\",\"cons.conf.idx\",\"age\",\"campaign\",\"previous\"]\n",
    "df_with_outliers=df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cons.price.idx**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x='y', y=\"cons.price.idx\", data=df_with_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no outliers for this feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nr.employed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='y', y=\"nr.employed\", data=df_with_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no outliers for this feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cons.conf.idx**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='y', y=\"cons.conf.idx\", data=df_with_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some unusual results in the target variable \"no\", but these do not significantly exceed the upper limit. \n",
    "\n",
    "Then, they fit the upper bound of the target variable \"yes\". \n",
    "\n",
    "Therefore, we chose to leave it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='y', y=\"age\", data=df_with_outliers)\n",
    "sns.boxplot(x='y', y=\"campaign\", data=df_with_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have outliers as max('age') and max('campaign') > 1.5Q3('age') and >1.5Q3('campaign') respectively.\n",
    "\n",
    "But we also see that the value of these outliers are not so unrealistic (max('age')=98 and max('campaign')=56).\n",
    "\n",
    "Hence, we need not remove them since the prediction model should represent the real world. \n",
    "\n",
    "This improves the generalizability of the model and makes it robust for real world situations. \n",
    "\n",
    "The outliers, therefore, are not removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"previous\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='y', y=\"previous\", data=df_with_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This variable has many unusual results, from the database, The unusual results belong to many calls made to a customer and therefore the outlierresults are much higher.\n",
    "\n",
    "We decided to sift the top results that exceed 3 times the upper limit, leaving the other results less than the top limit.\n",
    "we will do it at 5.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers=df_with_outliers[[\"previous\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4.2. Normalize features <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler   \n",
    "numeric_df = df.select_dtypes(exclude=\"object\")\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "normalized_df_data =scaler.fit_transform(numeric_df.values)\n",
    "df_scaled=pd.DataFrame(normalized_df_data,columns=numeric_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>5. Outlier Detection <h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5.1. Clustrering by DBSCAN <h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The optimal value for epsilon will be found at the point of maximum curvature.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "neigh = NearestNeighbors(n_neighbors=2)\n",
    "nbrs = neigh.fit(df_scaled)\n",
    "distances, indices = nbrs.kneighbors(df_scaled)\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]\n",
    "plt.plot(distances)\n",
    "plt.title(\"Find  the  optimal \"+r'$  \\varepsilon$',fontsize='xx-large', fontweight='bold')\n",
    "plt.ylabel(\"epsilon\")\n",
    "plt.xlabel(\"Feature unique values\")\n",
    "plt.plot([31825], [0.65], 'ro')\n",
    "plt.annotate('Optimal '+r'$\\varepsilon$', (31825,0.65),\n",
    "            xytext=(0.8, 0.9), textcoords='axes fraction',\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "            fontsize=16,\n",
    "            horizontalalignment='right', verticalalignment='top')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eps = the best epsilon is at the \"elbow\" of NearestNeighbors graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "db = DBSCAN(eps=0.65,min_samples=5).fit(df_scaled)\n",
    "labels=db.labels_\n",
    "clusterNum=len(set(labels))\n",
    "print(\"number of clusters is \"+str(clusterNum))\n",
    "noise=np.count_nonzero(labels == -1)\n",
    "noise_percentage=round(100*noise/df_scaled.shape[0],0)\n",
    "print(\"Number of outliers is \"+str(noise)+ \", Noise accounts for \"+str(noise_percentage)+\"%  of the total dataset\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=df_scaled\n",
    "df_DBSCAN=df.copy()\n",
    "df_DBSCAN[\"cluster_Db\"]=labels\n",
    "df_DBSCAN = df_DBSCAN[df_DBSCAN.cluster_Db != -1]\n",
    "df_DBSCAN=df_DBSCAN.drop(\"cluster_Db\",axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5.2. Multiple Clustrers by DBSCAN <h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found out that the best epsilon is just above the \"elbow\" - not that much noisy (about 15%) but lots of clusters that can define us different groups with different features.\n",
    "**Lots of clusters means low number of noise, therefore low number of outliers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5.3. Another method to remove outliers - LOF <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "z = np.abs(stats.zscore(df_outliers))\n",
    "#define a threshold to identify an outlier\n",
    "threshold = 3\n",
    "df_outliers=df_outliers[(z < threshold).all(axis=1)]\n",
    "Num_outliers_2nd=df_with_outliers.shape[0]-df_outliers.shape[0]\n",
    "print(\"outliers by each feature boxplot: \"+str(Num_outliers_2nd))\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "# fit the model for outlier detection (default)\n",
    "n_outliers = Num_outliers_2nd\n",
    "ground_truth = np.ones(len(df_scaled), dtype=int)\n",
    "ground_truth[-n_outliers:] = -1\n",
    "LOF = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "# use fit_predict to compute the predicted labels of the training samples\n",
    "# (when LOF is used for outlier detection, the estimator has no predict,\n",
    "# decision_function and score_samples methods).\n",
    "y_pred = LOF.fit_predict(df_scaled)\n",
    "n_errors = (y_pred != ground_truth).sum()\n",
    "X_scores = LOF.negative_outlier_factor_\n",
    "LOF_outliers=np.count_nonzero(y_pred == -1)\n",
    "LOF_outliers_percentage=round(100*np.count_nonzero(y_pred == -1)/df_scaled.shape[0],0)\n",
    "print(\"Number of outliers by LOF is \"+str(LOF_outliers)+ \", Noise accounts for \"+str(LOF_outliers_percentage)+\"%  of the total dataset\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can learn that local outliers (from each feature) does not predict the total outliers of the whole dataset by the combination of features.\n",
    "That is why we have more outliers by LOF **and** DBSCAN than that shown in the boxplots above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>6. Predictive Models <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% 6. Predictive Models\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from pprint import pprint\n",
    "from sklearn.utils import resample\n",
    "kfold = model_selection.KFold(n_splits=3)\n",
    "y=df_DBSCAN.y\n",
    "X=df_DBSCAN.drop(\"y\",axis=1)\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=0) #80/20 split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is imbalanced, therefore we used Upsampling. Due to overfitting, **we decided to drop this option**."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_train_no_yes=y_train.value_counts()\n",
    "df_majority = df_DBSCAN[df_DBSCAN.y==0]\n",
    "df_minority = df_DBSCAN[df_DBSCAN.y==1]\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=y_int(train_no_yes[0]/2),    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "df_upsampled.y.value_counts()\n",
    "y_upsample=df_upsampled.y\n",
    "X_upsample=df_upsampled.drop(\"y\",axis=1)\n",
    "x_train_up, x_test_up, y_train_up, y_test_up = model_selection.train_test_split(X_upsample, y_upsample, test_size=0.2, random_state=0) #80/20 split\n",
    "x_train=x_train_up\n",
    "y_train= y_train_up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>6.1. Random Forest <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "#explore the hyperparameters of this classifier\n",
    "pprint(rfc.get_params())\n",
    "# Number of trees in random forest\n",
    "n_estimators =[100,200,300]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [3,6,8]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 8]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)\n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rfc,\n",
    "                               param_distributions = random_grid,\n",
    "                               n_iter = 100, cv = kfold, verbose=2,\n",
    "                               random_state=42, n_jobs = -1)\n",
    "rf_random.fit(x_train, y_train)\n",
    "print(\"best parameters are:\")\n",
    "print(rf_random.best_params_)\n",
    "rf_best_random = rf_random.best_estimator_\n",
    "prediction_RF = rf_best_random.predict(x_test)\n",
    "print(classification_report(y_test, prediction_RF,target_names=[\"no\",\"yes\"]))\n",
    "CM_RF=confusion_matrix(y_test, prediction_RF)\n",
    "df_cm = pd.DataFrame(CM_RF, index = [\"Predicted No\",\"Predicted Yes\"],\n",
    "                  columns = [\"Actual No\",\"Actual Yes\"])\n",
    "plt.figure()\n",
    "sns.heatmap(df_cm,cmap=cmap, annot=True)\n",
    "#AUC\n",
    "probs_RF = rf_random.predict_proba(x_test)\n",
    "preds_RF = probs_RF[:,1]\n",
    "fprrfc, tprrfc, thresholdrfc = metrics.roc_curve(y_test, preds_RF)\n",
    "roc_aucrfc = metrics.auc(fprrfc, tprrfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>6.2. ADABOOST <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "ADA = AdaBoostClassifier()\n",
    "#explore the hyperparameters of this classifier\n",
    "pprint(ADA.get_params())\n",
    "#learning rate shrinks the contribution of each tree by learning_rate.\n",
    "learning_rate = [0.05,0.1,0.2]\n",
    "#algorithm ===================================================================\n",
    "# If ‘SAMME.R’ then use the SAMME.R real boosting algorithm.\n",
    "# base_estimator must support calculation of class probabilities.\n",
    "# If ‘SAMME’ then use the SAMME discrete boosting algorithm.\n",
    "# The SAMME.R algorithm typically converges faster than SAMME,\n",
    "# achieving a lower test error with fewer boosting iterations.\n",
    "# =============================================================================\n",
    "algorithm = [\"SAMME\", \"SAMME.R\"]\n",
    "n_estimators = [50,100,200]\n",
    "#The base estimator from which the boosted ensemble is built\n",
    "base_estimator= [DecisionTreeClassifier(max_depth=int(x)) for x in [3,6,8]]\n",
    "base_estimator.append(None)\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'algorithm': algorithm,\n",
    "               'base_estimator': base_estimator, \n",
    "               'learning_rate':learning_rate}\n",
    "pprint(random_grid)\n",
    "# search across 100 different combinations, and use all available cores\n",
    "ADA_random = RandomizedSearchCV(estimator = ADA,\n",
    "                               param_distributions = random_grid,\n",
    "                               n_iter = 100, cv = kfold, verbose=2,\n",
    "                               random_state=42, n_jobs = -1)\n",
    "ADA_random.fit(x_train, y_train)\n",
    "print(\"Best parameters are:\")\n",
    "print(ADA_random.best_params_)\n",
    "ADA_best_random = ADA_random.best_estimator_\n",
    "predictions_ADA = ADA_best_random.predict(x_test)\n",
    "CM_ADA=confusion_matrix(y_test, predictions_ADA)\n",
    "df_cm = pd.DataFrame(CM_ADA, index = [\"Predicted No\",\"Predicted Yes\"],\n",
    "                  columns = [\"Actual No\",\"Actual Yes\"])\n",
    "plt.figure()\n",
    "sns.heatmap(df_cm,cmap=cmap, annot=True)\n",
    "print(classification_report(y_test, predictions_ADA,target_names=[\"no\",\"yes\"]))\n",
    "#AUC\n",
    "probs_ADA = ADA_random.predict_proba(x_test)\n",
    "preds_ADA = probs_ADA[:,1]\n",
    "fprADA, tprADA, thresholdADA = metrics.roc_curve(y_test, preds_ADA)\n",
    "roc_aucADA = metrics.auc(fprADA, tprADA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>6.3. Gradient Boosting <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "grd = GradientBoostingClassifier()\n",
    "#explore the hyperparameters of this classifier\n",
    "pprint(grd.get_params())\n",
    "# Number of trees in random forest\n",
    "n_estimators = [100,200,300]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt',\"log2\"]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [3,6,8]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 8]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "#learning rate shrinks the contribution of each tree by learning_rate.\n",
    "learning_rate=[0.05,0.1,0.2]\n",
    "#loss function to be optimized\n",
    "loss=[\"deviance\", \"exponential\"]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "                'max_features': max_features,\n",
    "                'max_depth': max_depth,\n",
    "                'min_samples_split': min_samples_split,\n",
    "                'min_samples_leaf': min_samples_leaf,\n",
    "                'loss' : loss,\n",
    "                'learning_rate': learning_rate}\n",
    "pprint(random_grid)\n",
    "#search across 100 different combinations, and use all available cores\n",
    "grd_random = RandomizedSearchCV(estimator = grd,\n",
    "                                param_distributions = random_grid,\n",
    "                                n_iter = 100, cv = kfold, verbose=2,\n",
    "                                random_state=42, n_jobs = -1)\n",
    "grd_random.fit(x_train, y_train)\n",
    "grd_random.best_params_\n",
    "grd_best_random = grd_random.best_estimator_\n",
    "predictions_grd = grd_best_random.predict(x_test)\n",
    "CM_grd=confusion_matrix(y_test, predictions_grd)\n",
    "df_cm = pd.DataFrame(CM_grd, index = [\"Predicted No\",\"Predicted Yes\"],\n",
    "                  columns = [\"Actual No\",\"Actual Yes\"])\n",
    "print(classification_report(y_test, predictions_grd,target_names=[\"no\",\"yes\"]))\n",
    "#AUC\n",
    "probs_grd = grd_random.predict_proba(x_test)\n",
    "preds_grd = probs_grd[:,1]\n",
    "fprgrd, tprgrd, thresholdgrd = metrics.roc_curve(y_test, preds_grd)\n",
    "roc_aucgrd = metrics.auc(fprgrd, tprgrd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AUC Curves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.title('Predictive models RUC Comparison',fontsize=20)\n",
    "plt.ylabel('True Positive Rate',fontsize=20)\n",
    "plt.xlabel('False Positive Rate',fontsize=15)\n",
    "plt.plot(fprgrd, tprgrd, label = 'Gradient Boosting AUC = %0.3f' % roc_aucgrd)\n",
    "plt.plot(fprrfc, tprrfc, label = 'Random Forest AUC = %0.3f' % roc_aucrfc)\n",
    "plt.plot(fprADA, tprADA, label = 'ADABOOST AUC = %0.3f' % roc_aucADA)\n",
    "plt.legend(loc = 'lower right', prop={'size': 16})\n",
    "print(\"---Total run time: %s minutes ---\" % (round(time.time()/60 - start_time/60,2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
