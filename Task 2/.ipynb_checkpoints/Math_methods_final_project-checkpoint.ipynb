{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h1><u>Math methods for engineers - Pulsar Stars Classification</u><h1>\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "Ophir Shurany ID: 304867716\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2><u>Project Description:</u><h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Pulsars are a rare type of Neutron star that produce radio emission detectable here on Earth. They are of considerable scientific interest as probes of space-time, the inter-stellar medium, and states of matter.\n",
    "\n",
    "As pulsars rotate, their emission beam sweeps across the sky, and when this crosses our line of sight, produces a detectable pattern of broadband radio emission. As pulsars\n",
    "rotate rapidly, this pattern repeats periodically. Thus pulsar search involves looking for periodic radio signals with large radio telescopes.\n",
    "\n",
    "Each pulsar produces a slightly different emission pattern, which varies slightly with each rotation. Thus a potential signal detection known as a 'candidate', is averaged over many rotations of the pulsar, as determined by the length of an observation. In the absence of additional info, each candidate could potentially describe a real pulsar. However in practice almost all detections are caused by radio frequency interference (RFI) and noise, making legitimate signals hard to find.\n",
    "\n",
    "Machine learning tools are now being used to automatically label pulsar candidates to facilitate rapid analysis. Classification systems in particular are being widely adopted,\n",
    "which treat the candidate data sets as binary classification problems. Here the legitimate pulsar examples are a minority positive class, and spurious examples the majority negative class. At present multi-class labels are unavailable, given the costs associated with data annotation.\n",
    "\n",
    "The data set shared here contains 16,259 spurious examples caused by RFI/noise, and 1,639 real pulsar examples. These examples have all been checked by human annotators.\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "Each candidate is described by 8 continuous variables, and a single class variable. The first four are simple statistics obtained from the integrated pulse profile . This is an array of continuous variables that describe a longitude-resolved version of the signal that has been averaged in both time and frequency. The remaining four variables are similarly obtained from the DM-SNR curve. These are summarised below:\n",
    "\n",
    "1. Mean of the integrated profile.\n",
    "2. Standard deviation of the integrated profile.\n",
    "3. Excess kurtosis of the integrated profile.\n",
    "4. Skewness of the integrated profile.\n",
    "5. Mean of the DM-SNR curve.\n",
    "6. Standard deviation of the DM-SNR curve.\n",
    "7. Excess kurtosis of the DM-SNR curve.\n",
    "8. Skewness of the DM-SNR curve.\n",
    "9. Class\n",
    "\n",
    "[HTRU 2 Summary](https://archive.ics.uci.edu/ml/datasets/HTRU2#)\n",
    "\n",
    "17,898 total examples.\n",
    "\n",
    "1,639 positive examples.\n",
    "\n",
    "16,259 negative examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><u>Imbalanced dataset predictions models</u><h2>\n",
    "\n",
    "    1. Random Under Sampling\n",
    "    2. Random Over Sampling\n",
    "    3. SMOTE\n",
    "    4. ADASYN\n",
    "    5. Combine approach \n",
    "more info can be explained in [this link](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import  train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.close('all')\n",
    "import time\n",
    "start_time = time.time()\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_curve,auc,fbeta_score\n",
    "from sklearn.metrics import f1_score,v_measure_score,silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import cluster\n",
    "from sklearn.cluster import KMeans,AgglomerativeClustering\n",
    "from sklearn.metrics import jaccard_score,fowlkes_mallows_score,precision_recall_curve,log_loss\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "import matplotlib.cm as cm\n",
    "sns.set()\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Create dataframe\n",
    "data = pd.read_csv(\"pulsar_stars.csv\")\n",
    "#view first 5 rows in df\n",
    "data.head()\n",
    "#presenting all columns, number of rows and type\n",
    "data.info()\n",
    "#feature statistics for numerical categories\n",
    "data.describe()\n",
    "#Majority class is 0 (Not a pulsar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = list(data.columns)\n",
    "features = cols\n",
    "features.remove('target_class')\n",
    "# Normalization\n",
    "X=data[features]\n",
    "X = StandardScaler().fit_transform(X)\n",
    "Y=data.target_class\n",
    "#For this classification problem, I choose KNN 3 neighbors. \n",
    "model = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dummysampler is for measuring cross validation of the unbalanced data\n",
    "- The Combine approach is called SMOTETomek, a class thats performs over-sampling using SMOTE and cleaning using Tomek links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummySampler:\n",
    "    def sample(self, X, y):\n",
    "        return X, y\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    def fit_resample(self, X, y):\n",
    "        return self.sample(X, y)\n",
    "# prepare samplers\n",
    "RUS=RandomUnderSampler()\n",
    "ADASYN=ADASYN()\n",
    "ROS= RandomOverSampler()\n",
    "SMOTE=SMOTE()\n",
    "Combine=SMOTETomek()\n",
    "Samplers = []\n",
    "Samplers.append(('Original', DummySampler()))\n",
    "Samplers.append(('RUS', RUS))\n",
    "Samplers.append(('ROS', ROS))\n",
    "Samplers.append(('ADASYN', ADASYN))\n",
    "Samplers.append(('SMOTE', SMOTE))\n",
    "Samplers.append(('Combine', Combine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The very simple metric to measure classification is basic accuracy i.e. ratio of correct predictions to the total number of samples in dataset. However, in the case of imbalanced classes this metric can be misguiding, as high metrics doesn’t show prediction capacity for the minority class.\n",
    "\n",
    "I will now compare the results of the different models on the validation set using the following models:\n",
    "- Accuracy*\n",
    "- F1 Measure\n",
    "- Precision\n",
    "- Recall\n",
    "- ROC-AUC\n",
    "\n",
    "*Precision is an unsuitable measure of this problem, below I will explain the reasons why\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each sampler in turn\n",
    "score = ['accuracy', 'f1',\"precision\",\"recall\"] #Different measures\n",
    "RANDOM_STATE=0\n",
    "for scoring in score:\n",
    "    print(\"****************\"+scoring+\"**************\")\n",
    "    results = []\n",
    "    names = []\n",
    "    predictions=[]\n",
    "    f1_score_tot=[];f2_score_tot=[]\n",
    "    fpr_tot=[];tpr_tot=[];roc_auc_tot=[]\n",
    "    for name, sampler in Samplers:\n",
    "        print(\"-----------\"+name+\"---------\")\n",
    "        X_sampled, Y_sampled = sampler.fit_resample(X,Y)\n",
    "        print(\"The number of samples in \",name,\" dataset is\" ,X_sampled.shape[0])\n",
    "        print(Y_sampled.value_counts())\n",
    "        print(\"Ratio samples in \",name,\" dataset target values is\",round(Y_sampled.value_counts()[1]/Y_sampled.value_counts()[0],3))\n",
    "#Now I will randomly split the data so that 10% is saved for validation and 90% for training and test\n",
    "        X_train_test, X_val, Y_train_test, Y_val =train_test_split(X_sampled, Y_sampled, test_size=0.1, random_state=42)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_train_test, Y_train_test, test_size=0.2, random_state=42)\n",
    "\n",
    "        cv_results = cross_val_score(model, X_val, Y_val,cv=10, scoring=scoring)\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "        print(\"cross validation score - \",msg)\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred=model.predict(X_test)\n",
    "        predictions.append(y_pred)\n",
    "        probs= model.predict_proba(X_test)\n",
    "        preds = probs[:,1]\n",
    "        #F2 score\n",
    "        f1_measure=f1_score(y_test, y_pred)\n",
    "        f1_score_tot.append(f1_measure)\n",
    "        f2_score=fbeta_score(y_test, y_pred, beta=2)\n",
    "        f2_score_tot.append(f2_score)\n",
    "        #ROC-AUC\n",
    "        fpr, tpr, threshold = roc_curve(y_test, y_pred)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        fpr_tot.append(fpr);tpr_tot.append(tpr);roc_auc_tot.append(roc_auc)\n",
    "#    print(classification_report(y_test, model.predict(X_test),target_names=[\"no\",\"yes\"]))\n",
    "#    CM=confusion_matrix(y_test, y_pred)\n",
    "#    print(pd.DataFrame(CM, index = [\"Predicted No\",\"Predicted Yes\"],\n",
    "#              columns = [\"Actual No\",\"Actual Yes\"]))\n",
    "    # boxplot algorithm comparison\n",
    "    fig1 = plt.figure(figsize=(14, 9))\n",
    "    plt.tight_layout()\n",
    "    fig1.suptitle('Samplers Comparison by '+scoring)\n",
    "    ax1 = fig1.add_subplot(111)\n",
    "    plt.boxplot(results)\n",
    "    ax1.set_xticklabels(names)\n",
    "    ax1.set_ylabel(scoring+\" [%]\")\n",
    "\n",
    "plt.figure(figsize=(14, 9))\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.title('Predictive models RUC Comparison',fontsize=20)\n",
    "plt.ylabel('True Positive Rate',fontsize=20)\n",
    "plt.xlabel('False Positive Rate',fontsize=15)\n",
    "plt.tight_layout()\n",
    "for i in range(len(names)):\n",
    "    plt.plot(fpr_tot[i], tpr_tot[i], label = names[i]+' AUC = %0.3f' % roc_auc_tot[i])\n",
    "    plt.legend(loc = 'lower right', prop={'size': 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samplers comparison by accuracy:\n",
    "As mention before, the accuracy of unbalanced data (Original) has very high accuracy, compare to balanced datasets, but also has a very low values at F1 measure, mainly because low recall (can be seen in boxplot 4 **\"Samplers comparison by recall\"**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any $\\beta$ score the equation is: $$ F_\\beta = (1+\\beta^2)\\cdot \\frac{precision \\cdot recall}{(\\beta^2 \\cdot precision) + recall}\\equiv  \\frac{(1+\\beta^2)\\cdot True Positive}{(1+\\beta^2)\\cdot True Positive+\\beta^2 \\cdot False Negative + False Positive}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for F2, $\\beta = 2 $, and so $$ F_2 = 5\\cdot \\frac{precision \\cdot recall}{(4 \\cdot precision) + recall}\\equiv  \\frac{5\\cdot True Positive}{5\\cdot True Positive+4 \\cdot False Negative + False Positive}  $$\n",
    "It can be seen that F2 measure weighs recall higher than precision (by placing more emphasis on false negatives). \n",
    "This makes the F2 score more suitable in certain applications where it’s more important to classify correctly as many positive samples as possible, rather than maximizing the number of correct classifications, as in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(zip(f1_score_tot,f2_score_tot)),columns =['F1 measure', 'F2 measure'],index=names) .T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a conclusion I would recommend to use accuracy only if the classes are perfectly balanced, and otherwise use F2 and ROC-AUC. It is also useful to see ratio of positives and negative estimation via precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><u>Clustering the  original dataset<u><h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Create dataframe\n",
    "data = pd.read_csv(\"pulsar_stars.csv\")\n",
    "#view first 5 rows in df\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would for first part of our implementation, treat it as a unlabelled dataset and try to run clustering algorithms to find out the distinct group of data points namely:\n",
    "\n",
    "1. [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis\\)\n",
    "2. [Agglomerative Clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)\n",
    "3. [KMeans](https://en.wikipedia.org/wiki/K-means_clustering)\n",
    "\n",
    "PCA - Principal Component Analysis\n",
    "It is a precursor step to any analysis that we may subject to our dataset. The above dataset has decent dimensional feature space consisting of 8 features. In such a high-dimensional space, Euclidean distances tend to become inflated and meaningless. This can severely impact our algorithms performance. Such a situation demands more data to train our model and this problem is called the 'Curse of Dimensionality'.\n",
    "\n",
    "The PCA algorithm solves this problem by finding out the features that explain the maximum variance. So, instead of training our models over 8 features we will be training them over 2 features that explain the maximum variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(data.columns)\n",
    "features = cols\n",
    "features.remove('target_class')\n",
    "# Normalization\n",
    "X=data[features]\n",
    "X = StandardScaler().fit_transform(X)\n",
    "Y=data.target_class\n",
    "# Split dataset to 60% training and 40% testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state=0)\n",
    "# First we reduce the data to two dimensions using PCA to capture variation\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(X)\n",
    "#print(reduced_data[:10])  # print upto 10 elements\n",
    "reduced_data.shape\n",
    "principalDf = pd.DataFrame(data = reduced_data, columns = ['principal component 1', 'principal component 2'])\n",
    "finalDf = pd.concat([principalDf, data[['target_class']]], axis = 1)\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "PCA1='principal component 1 - '+str(100*round(pca.explained_variance_ratio_[0],2))+\"% variance\"\n",
    "PCA2='principal component 2 - '+str(100*round(pca.explained_variance_ratio_[1],2))+\"% variance\"\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel(PCA1, fontsize = 15)\n",
    "ax.set_ylabel(PCA2, fontsize = 15)\n",
    "ax.set_title('2 component PCA by Kmeans', fontsize = 20)\n",
    "targets = [1,0]\n",
    "colors = ['b', 'r']\n",
    "pca.explained_variance_ratio_\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['target_class'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color,alpha=0.5)\n",
    "ax.grid()\n",
    "kmeans = cluster.KMeans(n_clusters=2)\n",
    "kmeans.fit(reduced_data)\n",
    "centroids = kmeans.cluster_centers_\n",
    "ax.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='x', s=169, linewidths=3,\n",
    "            color='k', zorder=10)\n",
    "ax.legend([\"Positive\",\"Negative\",\"Centroid\"])\n",
    "print(\" Together, the two components contain\",str(100*round(sum(pca.explained_variance_ratio_),2)),\"% of the information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Average silhouette method](https://en.wikipedia.org/wiki/Silhouette_(clustering)) computes the average silhouette of observations for different values of k. The optimal number of clusters k is the one that maximize the average silhouette over a range of possible values for k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_KMeans=[]\n",
    "results_AC_n=[]\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    AC_n=AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    Kmeans_n = KMeans(n_clusters=n_clusters)\n",
    "    clusterers=[Kmeans_n,AC_n]\n",
    "    cluster_names=[\"KMeans\",\"Agglomerative Clustering\"]\n",
    "    for clusterer in clusterers:\n",
    "        cluster_labels = clusterer.fit_predict(reduced_data)\n",
    "        # The silhouette_score gives the average value for all the samples.\n",
    "        # This gives a perspective into the density and separation of the formed\n",
    "        # clusters\n",
    "        silhouette_avg = silhouette_score(reduced_data, cluster_labels)\n",
    "        if cluster_names[clusterers.index(clusterer)]==\"KMeans\":\n",
    "            results_KMeans.append(silhouette_avg)\n",
    "        else:\n",
    "            results_AC_n.append(silhouette_avg)\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(range_n_clusters,results_KMeans,'-og', label='Kmeans')\n",
    "ax.plot(range_n_clusters,results_AC_n,'-or', label='Agglomerative Clustering')\n",
    "ax.set_title(\"The silhouette plot for the various clusters\")\n",
    "ax.set_ylabel(\"The silhouette coefficient values\")\n",
    "ax.set_xticks(range_n_clusters)\n",
    "ax.set_xticklabels(range_n_clusters)\n",
    "ax.set_xlabel(\"# Clusters\")\n",
    "ax.legend()\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "he optimal number of clusters k is the one that maximize the average silhouette over a range of possible values for k=2.\n",
    "\n",
    "Our data has groudtruth, we will use it to for the following external validity measures:\n",
    "\n",
    "1. Pair-wise measure:\n",
    "\n",
    "    1.1. [Rand statistics](https://en.wikipedia.org/wiki/Rand_index)\n",
    "    \n",
    "    1.2. [Jaccard coefficient](https://en.wikipedia.org/wiki/Jaccard_index)\n",
    "    \n",
    "    1.3. [Fowlkes-Mallows](https://en.wikipedia.org/wiki/Fowlkes%E2%80%93Mallows_index)\n",
    "    \n",
    "2. Matching based measures - [F-Measure](https://en.wikipedia.org/wiki/F1_score)\n",
    "3. Entropy based measure - [Conditional Entropy](https://en.wikipedia.org/wiki/Conditional_entropy)\n",
    "\n",
    "Total number of 5 different external validity measures.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reduced_data, Y, test_size=0.2, random_state=42)\n",
    "AC_n=AgglomerativeClustering(n_clusters=2)\n",
    "Kmeans_n = KMeans(n_clusters=2)\n",
    "clusterers=[Kmeans_n,AC_n]\n",
    "cluster_names=[\"KMeans\",\"Agglomerative Clustering\"]\n",
    "fit_results=[]\n",
    "for clusterer in clusterers:   \n",
    "    clusterer.fit(X_train, y_train)\n",
    "    y_pred=clusterer.fit_predict(X_test)\n",
    "    Rand=adjusted_rand_score(y_test, y_pred)\n",
    "    jaccard=jaccard_score(y_test, y_pred)\n",
    "    FM=fowlkes_mallows_score(y_test, y_pred)\n",
    "    cond_entropy=v_measure_score(y_test, y_pred)\n",
    "    F_score=f1_score(y_test, y_pred)\n",
    "    fit_results.append([Rand,jaccard,FM,F_score,cond_entropy])\n",
    "results_table=pd.DataFrame(fit_results,\n",
    "                           columns=['Rand', 'jaccard', 'FM',\"cond_entropy\",\"F_score\"],\n",
    "                           index=cluster_names).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=2\n",
    "colors = ['r','b']\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,7.5))\n",
    "\n",
    "km = KMeans(n_clusters= n , random_state=0)\n",
    "y_km = km.fit_predict(reduced_data)\n",
    "\n",
    "for i in range(n):\n",
    "    ax1.scatter(reduced_data[y_km==i,0], reduced_data[y_km==i,1], c=colors[i], marker='o',alpha=0.5)   \n",
    "ax1.set_title('K-means Clustering')\n",
    "\n",
    "\n",
    "ac = AgglomerativeClustering(n_clusters=n, affinity='euclidean', linkage='complete')\n",
    "y_ac = ac.fit_predict(reduced_data)\n",
    "\n",
    "for i in range(n):\n",
    "    ax2.scatter(reduced_data[y_ac==i,0], reduced_data[y_ac==i,1], c=colors[i], marker='o',alpha=0.5)\n",
    "ax2.set_title('Agglomerative Clustering')\n",
    "# Put a legend below current axis\n",
    "plt.legend([\"Negative\",\"Positive\"])\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- %s minutes ---\" % (round(time.time()/60 - start_time/60,2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
